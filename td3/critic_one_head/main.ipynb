{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fifty-motorcycle",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import gym\n",
    "import numpy as np\n",
    "import os\n",
    "import pickle\n",
    "import torch\n",
    "\n",
    "from td3 import TD3Agent\n",
    "from main import train, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "harmful-harmony",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_train_episodes = 3000\n",
    "num_memory_fill_episodes = 10\n",
    "num_test_episodes = 100\n",
    "memory_capacity = 10000\n",
    "update_freq = 2\n",
    "batchsize = 64\n",
    "discount = 0.99\n",
    "tau = 0.005\n",
    "policy_noise_std = 0.2\n",
    "policy_noise_clip = 0.5\n",
    "exploration_noise = 0.1\n",
    "actor_lr = 1e-3\n",
    "critic_lr = 1e-3\n",
    "results_folder = None\n",
    "env_name = 'LunarLanderContinuous-v2'\n",
    "train_mode = True\n",
    "cuda_device = 'cuda:0'\n",
    "train_seed = 12321\n",
    "test_seed = [456, 12,985234, 123, 3202]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "limiting-textbook",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(cuda_device if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "specified-wireless",
   "metadata": {},
   "outputs": [],
   "source": [
    "if results_folder is None:\n",
    "    results_folder = 'results/{}_disc{}_actorlr{}_criticlr{}_tau{}_noisestd{}_noiseclip{}_expl{}_d{}'.format(env_name, discount, actor_lr, critic_lr, tau, policy_noise_std, policy_noise_clip, exploration_noise, update_freq)\n",
    "\n",
    "if not os.path.exists('results'):\n",
    "    os.mkdir('results')\n",
    "\n",
    "if not os.path.exists(results_folder):\n",
    "    os.mkdir(results_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "excess-peripheral",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory filled:  1190\n",
      "Ep: 0 | Ep reward: -180.31215696136587 | Moving avg: -180.31215696136587\n",
      "Ep: 1 | Ep reward: -291.6474859133323 | Moving avg: -235.97982143734907\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-686d8a488ac1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     28\u001b[0m                 \u001b[0mbatchsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatchsize\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m                 \u001b[0mexploration_noise\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mexploration_noise\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m                 results_folder=results_folder)\n\u001b[0m\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m     \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/rl-series/td3/critic_one_head/main.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(env, td3_agent, epochs_train, epochs_fill_memory, batchsize, exploration_noise, results_folder)\u001b[0m\n\u001b[1;32m     85\u001b[0m             \u001b[0mtd3_agent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmemory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# store the interaction in the replay buffer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 87\u001b[0;31m             \u001b[0mtd3_agent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcurrent_iteration\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtotal_steps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatchsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatchsize\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# update the networks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     88\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m             \u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext_state\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/rl-series/td3/critic_one_head/td3.py\u001b[0m in \u001b[0;36mlearn\u001b[0;34m(self, current_iteration, batchsize)\u001b[0m\n\u001b[1;32m    149\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcritic2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    150\u001b[0m         \u001b[0mcritic_loss2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 151\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcritic2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    152\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    153\u001b[0m         \u001b[0;31m# delayed policy and target updates\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/miniconda3/envs/blogpytorch/lib/python3.5/site-packages/torch/optim/adam.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m     92\u001b[0m                 \u001b[0;31m# Decay the first and second moment running average coefficient\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m                 \u001b[0mexp_avg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbeta1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mbeta1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 94\u001b[0;31m                 \u001b[0mexp_avg_sq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbeta2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddcmul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mbeta2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     95\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mamsgrad\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m                     \u001b[0;31m# Maintains the maximum of all 2nd moment running avg. till now\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "if train_mode:\n",
    "    os.environ['PYTHONHASHSEED']=str(train_seed)\n",
    "    np.random.seed(train_seed)\n",
    "    torch.manual_seed(train_seed)\n",
    "\n",
    "    env = gym.make(env_name)\n",
    "    env.seed(train_seed)\n",
    "    env.action_space.np_random.seed(train_seed)\n",
    "    \n",
    "    td3_agent = TD3Agent(state_dim=env.observation_space.shape[0], \n",
    "                                action_dim=env.action_space.shape[0],\n",
    "                                max_action=env.action_space.high[0], # clamp only works with numbers, not with arrays \n",
    "                                device=device, \n",
    "                                memory_capacity=memory_capacity, \n",
    "                                discount=discount, \n",
    "                                update_freq=update_freq, \n",
    "                                tau=tau, \n",
    "                                policy_noise_std=policy_noise_std, \n",
    "                                policy_noise_clip=policy_noise_clip, \n",
    "                                actor_lr=actor_lr, \n",
    "                                critic_lr=critic_lr, \n",
    "                                train_mode=train_mode)\n",
    "\n",
    "    train(env=env, \n",
    "                td3_agent=td3_agent, \n",
    "                epochs_train=num_train_episodes, \n",
    "                epochs_fill_memory=num_memory_fill_episodes, \n",
    "                batchsize=batchsize, \n",
    "                exploration_noise=exploration_noise,\n",
    "                results_folder=results_folder)\n",
    "\n",
    "    env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "floating-shape",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_test(seed):\n",
    "    print('=== TEST SEED: {} ==='.format(seed))\n",
    "    os.environ['PYTHONHASHSEED']=str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "\n",
    "    env = gym.make(env_name)\n",
    "    env.seed(seed)\n",
    "\n",
    "    td3_agent = TD3Agent(state_dim=env.observation_space.shape[0], \n",
    "                                    action_dim=env.action_space.shape[0],\n",
    "                                    max_action=env.action_space.high[0], # clamp only works with numbers, not with arrays \n",
    "                                    device=device,  \n",
    "                                    train_mode='False')\n",
    "    td3_agent.load(path=results_folder, model_name='best')\n",
    "\n",
    "    test(env=env,\n",
    "                td3_agent=td3_agent,\n",
    "                epochs_test=num_test_episodes,\n",
    "                seed=seed,\n",
    "                results_folder=results_folder)\n",
    "\n",
    "    env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "interesting-disabled",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== TEST SEED: 456 ===\n",
      "Ep: 0 | Ep reward: 106\n",
      "Ep: 1 | Ep reward: 93\n",
      "Ep: 2 | Ep reward: 151\n",
      "Ep: 3 | Ep reward: 179\n",
      "Ep: 4 | Ep reward: 110\n",
      "Ep: 5 | Ep reward: 141\n",
      "Ep: 6 | Ep reward: 82\n",
      "Ep: 7 | Ep reward: 174\n",
      "Ep: 8 | Ep reward: 247\n",
      "Ep: 9 | Ep reward: 124\n",
      "Ep: 10 | Ep reward: 140\n",
      "Ep: 11 | Ep reward: 92\n",
      "Ep: 12 | Ep reward: 83\n",
      "Ep: 13 | Ep reward: 94\n",
      "Ep: 14 | Ep reward: 140\n",
      "Ep: 15 | Ep reward: 152\n",
      "Ep: 16 | Ep reward: 157\n",
      "Ep: 17 | Ep reward: 172\n",
      "Ep: 18 | Ep reward: 125\n",
      "Ep: 19 | Ep reward: 121\n",
      "Ep: 20 | Ep reward: 99\n",
      "Ep: 21 | Ep reward: 138\n",
      "Ep: 22 | Ep reward: 99\n",
      "Ep: 23 | Ep reward: 110\n",
      "Ep: 24 | Ep reward: 93\n",
      "Ep: 25 | Ep reward: 125\n",
      "Ep: 26 | Ep reward: 134\n",
      "Ep: 27 | Ep reward: 113\n",
      "Ep: 28 | Ep reward: 187\n",
      "Ep: 29 | Ep reward: 162\n",
      "Ep: 30 | Ep reward: 223\n",
      "Ep: 31 | Ep reward: 157\n",
      "Ep: 32 | Ep reward: 130\n",
      "Ep: 33 | Ep reward: 227\n",
      "Ep: 34 | Ep reward: 136\n",
      "Ep: 35 | Ep reward: 83\n",
      "Ep: 36 | Ep reward: 549\n",
      "Ep: 37 | Ep reward: 140\n",
      "Ep: 38 | Ep reward: 113\n",
      "Ep: 39 | Ep reward: 162\n",
      "Ep: 40 | Ep reward: 108\n",
      "Ep: 41 | Ep reward: 125\n",
      "Ep: 42 | Ep reward: 110\n",
      "Ep: 43 | Ep reward: 93\n",
      "Ep: 44 | Ep reward: 103\n",
      "Ep: 45 | Ep reward: 84\n",
      "Ep: 46 | Ep reward: 162\n",
      "Ep: 47 | Ep reward: 110\n",
      "Ep: 48 | Ep reward: 138\n",
      "Ep: 49 | Ep reward: 175\n",
      "Ep: 50 | Ep reward: 91\n",
      "Ep: 51 | Ep reward: 85\n",
      "Ep: 52 | Ep reward: 144\n",
      "Ep: 53 | Ep reward: 171\n",
      "Ep: 54 | Ep reward: 83\n",
      "Ep: 55 | Ep reward: 107\n",
      "Ep: 56 | Ep reward: 135\n",
      "Ep: 57 | Ep reward: 218\n",
      "Ep: 58 | Ep reward: 109\n",
      "Ep: 59 | Ep reward: 84\n",
      "Ep: 60 | Ep reward: 176\n",
      "Ep: 61 | Ep reward: 133\n",
      "Ep: 62 | Ep reward: 100\n",
      "Ep: 63 | Ep reward: 90\n",
      "Ep: 64 | Ep reward: 84\n",
      "Ep: 65 | Ep reward: 133\n",
      "Ep: 66 | Ep reward: 143\n",
      "Ep: 67 | Ep reward: 171\n",
      "Ep: 68 | Ep reward: 98\n",
      "Ep: 69 | Ep reward: 239\n",
      "Ep: 70 | Ep reward: 141\n",
      "Ep: 71 | Ep reward: 135\n",
      "Ep: 72 | Ep reward: 162\n",
      "Ep: 73 | Ep reward: 83\n",
      "Ep: 74 | Ep reward: 143\n",
      "Ep: 75 | Ep reward: 193\n",
      "Ep: 76 | Ep reward: 212\n",
      "Ep: 77 | Ep reward: 163\n",
      "Ep: 78 | Ep reward: 166\n",
      "Ep: 79 | Ep reward: 124\n",
      "Ep: 80 | Ep reward: 87\n",
      "Ep: 81 | Ep reward: 153\n",
      "Ep: 82 | Ep reward: 133\n",
      "Ep: 83 | Ep reward: 252\n",
      "Ep: 84 | Ep reward: 86\n",
      "Ep: 85 | Ep reward: 155\n",
      "Ep: 86 | Ep reward: 139\n",
      "Ep: 87 | Ep reward: 105\n",
      "Ep: 88 | Ep reward: 275\n",
      "Ep: 89 | Ep reward: 170\n",
      "Ep: 90 | Ep reward: 148\n",
      "Ep: 91 | Ep reward: 82\n",
      "Ep: 92 | Ep reward: 122\n",
      "Ep: 93 | Ep reward: 173\n",
      "Ep: 94 | Ep reward: 136\n",
      "Ep: 95 | Ep reward: 116\n",
      "Ep: 96 | Ep reward: 242\n",
      "Ep: 97 | Ep reward: 195\n",
      "Ep: 98 | Ep reward: 205\n",
      "Ep: 99 | Ep reward: 231\n"
     ]
    }
   ],
   "source": [
    "run_test(test_seed[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "foreign-treaty",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
